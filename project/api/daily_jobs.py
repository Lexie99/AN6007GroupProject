# daily_jobs_api.py

import os
import time
import json
import redis
import threading
from datetime import datetime, timedelta
from flask import jsonify
from api.logs_backup import log_event

# Redis è¿æ¥
redis_host = os.getenv('REDIS_HOST', 'localhost')
redis_port = int(os.getenv('REDIS_PORT', 6379))
r = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)

# é…ç½®é¡¹
KEEP_DAYS = 365               # ä¿ç•™365å¤©çš„æ•°æ®
MAINTENANCE_DURATION = 3600   # ç»´æŠ¤æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1å°æ—¶
IS_MAINTENANCE = False        # å…¨å±€æ ‡è®°ï¼šæ˜¯å¦å¤„äºç»´æŠ¤æ¨¡å¼

def daily_jobs_api(app):
    @app.route('/stopserver', methods=['GET'])
    def stop_server():
        """
        å¯åŠ¨ç»´æŠ¤æ¨¡å¼ï¼š
        - è‹¥å½“å‰å·²åœ¨ç»´æŠ¤ï¼Œåˆ™è¿”å›é”™è¯¯
        - å¦åˆ™ï¼Œåˆ›å»ºä¸€ä¸ªåå°çº¿ç¨‹æ‰§è¡Œ run_maintenance()ï¼Œç„¶åç«‹å³è¿”å›
        """
        global IS_MAINTENANCE
        if IS_MAINTENANCE:
            return jsonify({'status': 'error', 'message': 'Already in maintenance mode.'}), 400

        try:
            # æ ‡è®°è¿›å…¥ç»´æŠ¤
            IS_MAINTENANCE = True

            # å¯åŠ¨åå°çº¿ç¨‹æ‰§è¡Œ run_maintenanceï¼ˆä¸é˜»å¡å½“å‰è¯·æ±‚ï¼‰
            maintenance_thread = threading.Thread(target=run_maintenance, daemon=True)
            maintenance_thread.start()

            return jsonify({'status': 'success', 'message': 'Server is in maintenance mode. Background job started.'}), 200
        except Exception as e:
            IS_MAINTENANCE = False
            return jsonify({'status': 'error', 'message': f'Failed to start maintenance mode: {str(e)}'}), 500


def run_maintenance():
    """
    åå°æ‰§è¡Œçš„ç»´æŠ¤ä»»åŠ¡ï¼š
    1. æ—¥å¿—è®°å½•è¿›å…¥ç»´æŠ¤æ¨¡å¼
    2. è®¡ç®—å¹¶å¤‡ä»½æ˜¨æ—¥ç”µè¡¨æ•°æ® -> process_daily_meter_readings()
    3. æ¨¡æ‹Ÿåœæœº:sleep(ç»´æŠ¤æ—¶é•¿)
    4. ç»´æŠ¤ç»“æŸåå¤„ç† pending -> process_pending_data()
    5. é€€å‡ºç»´æŠ¤æ¨¡å¼
    """
    
    log_event("daily_jobs", "Server entering maintenance mode...")

    # 1) è®¡ç®—å¹¶å¤‡ä»½æ˜¨æ—¥ç”µè¡¨æ•°æ®
    process_daily_meter_readings()

    # 2) æ¨¡æ‹Ÿåœæœº
    print(f"â³ Server in maintenance mode for {MAINTENANCE_DURATION / 60} minutes...")
    time.sleep(MAINTENANCE_DURATION)

    # 3) ç»´æŠ¤ç»“æŸåå¤„ç† pending
    process_pending_data()

    global IS_MAINTENANCE
    IS_MAINTENANCE = False
    log_event("daily_jobs", "Server maintenance completed.")


def process_daily_meter_readings():
    """
    è®¡ç®—æ˜¨æ—¥æ€»ç”¨ç”µé‡ï¼Œå­˜å…¥ Redis å¤‡ä»½
    """
    yesterday = (datetime.now() - timedelta(days=1)).date()
    start_timestamp = datetime(yesterday.year, yesterday.month, yesterday.day).timestamp()
    end_timestamp = start_timestamp + 86400

    backup_key = f"backup:meter_data:{yesterday}"
    total_processed = 0

    meter_keys = r.scan_iter("meter:*:history")
    for key in meter_keys:
        parts = key.split(":")
        if len(parts) < 3:
            continue
        meter_id = parts[1]

        # è·å–æ˜¨æ—¥èŒƒå›´å†…æ•°æ®
        readings = r.zrangebyscore(key, start_timestamp, end_timestamp)
        if len(readings) < 2:
            continue  # ä¸èƒ½è®¡ç®—å¢é‡

        data_points = []
        for raw in readings:
            try:
                data_points.append(json.loads(raw))
            except json.JSONDecodeError:
                print(f"[Backup] JSON decode error for meter {meter_id}: {raw}")
                continue

        # è®¡ç®—å¢é‡ (æœ€å - ç¬¬ä¸€ä¸ª)
        total_usage = float(data_points[-1]["reading_value"]) - float(data_points[0]["reading_value"])

        # å†™å…¥å¤‡ä»½ hash
        r.hset(
            backup_key,
            meter_id,
            json.dumps({
                "meter_id": meter_id,
                "date": str(yesterday),
                "total_usage": total_usage
            })
        )
        total_processed += 1

    print(f"ğŸ“Š [Backup] Processed {total_processed} meters for {yesterday}. Stored in {backup_key}.")

    # å¤‡ä»½å®Œæˆåå†æ¸…ç†æ—§æ•°æ®
    clean_old_data()


def clean_old_data():
    """
    åˆ é™¤è¶…è¿‡ KEEP_DAYS çš„å†å²è¯»æ•°
    """
    cutoff_date = datetime.now() - timedelta(days=KEEP_DAYS)
    cutoff_timestamp = cutoff_date.timestamp()
    deleted_records = 0

    meter_keys = r.scan_iter("meter:*:history")
    for key in meter_keys:
        deleted = r.zremrangebyscore(key, "-inf", cutoff_timestamp)
        deleted_records += deleted

    print(f"ğŸ—‘ï¸ [Clean] Deleted {deleted_records} old records (older than {KEEP_DAYS} days).")


def process_pending_data():
    """
    è‹¥ç»´æŠ¤æœŸé—´è¿˜è¦æ¥æ”¶æ–°è¯»æ•°å¹¶å†™åˆ° pending,è¿™é‡Œä¸€æ¬¡æ€§å¤„ç†å¹¶å†™å› historyã€‚
    """
    meter_keys = r.keys("meter:*:pending")
    processed_count = 0

    for key in meter_keys:
        parts = key.split(":")
        if len(parts) < 3:
            continue
        meter_id = parts[1]

        try:
            pending_data = r.lrange(key, 0, -1)
        except redis.RedisError as e:
            print(f"[Pending] Error retrieving data for meter {meter_id}: {str(e)}")
            continue

        if not pending_data:
            continue

        for record in pending_data:
            try:
                data = json.loads(record)
                dt_obj = datetime.fromisoformat(data["timestamp"])
                timestamp_unix = int(dt_obj.timestamp())

                # å†™åˆ° meter:{id}:history
                r.zadd(f"meter:{meter_id}:history", {record: timestamp_unix})
            except Exception as ex:
                print(f"[Pending] Error processing record: {record}, err={ex}")
                continue

        # æ¸…ç©º pending
        r.delete(key)
        processed_count += 1

    print(f"âœ… [Pending] Processed {processed_count} meters from pending.")
